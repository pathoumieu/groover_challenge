{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pa/.virtualenvs/groover/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# LGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('./data/raw/submission_history.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('./cc.fr.300.vec') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = sub.track_info.fillna('Inconnu').values\n",
    "\n",
    "MAX_NUM_WORDS = max([len(text) for text in texts])\n",
    "MAX_SEQUENCE_LENGTH  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38848 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "dataset = pd.read_csv('./data/preprocessed/merged_dataset.csv')\n",
    "X = dataset.drop(columns=['id', 'track_id', 'band_id', 'influencer_id', 'score'])\n",
    "y = dataset.score\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_data = X.filter(regex='_band')\n",
    "influencer_data = X.filter(regex='_influencer')\n",
    "\n",
    "influencer_kind = X.influencer_kind\n",
    "i_kind_idx = X.columns.get_loc('influencer_kind')\n",
    "\n",
    "b_data_idx = [X.columns.get_loc(c) for c in band_data]\n",
    "i_data_idx = [X.columns.get_loc(c) for c in influencer_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NN_model(\n",
    "    i_emb_dim=16, \n",
    "    b_emb_dim=16, \n",
    "    kind_emb_dim=8, \n",
    "    last_dense=16, \n",
    "    dropout=0.2,\n",
    "    activation='relu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Build simple MLP Neural Network.\n",
    "    \"\"\"\n",
    "    # Influencer embedding\n",
    "    influencer_input = Input(shape=[influencer_data.shape[1]], name=\"Influencer-Input\")\n",
    "    influencer_embedding = Dense(i_emb_dim, activation=activation, name=\"Influencer-Embedding\")(influencer_input)\n",
    "    \n",
    "    # Influencer kind categorical embedding\n",
    "    influencer_kind_input = Input(shape=[1], name=\"Influencer-Kind-Input\")\n",
    "    influencer_kind_emb = Embedding(\n",
    "        influencer_kind.nunique(), \n",
    "        kind_emb_dim, \n",
    "        name=\"Influencer-Kind-Embedding\"\n",
    "    )(influencer_kind_input)\n",
    "    \n",
    "    # Concatenate influencer emb with influencer kind emb to get full influencer emb\n",
    "    influencer_full_emb = Concatenate(name=\"Influencer-Full-Embedding\", axis=-1)(\n",
    "        [influencer_embedding, Flatten(name='Flatten')(influencer_kind_emb)]\n",
    "    )\n",
    "    \n",
    "    # Band embedding\n",
    "    band_input = Input(shape=[band_data.shape[1]], name=\"Band-Input\")\n",
    "    band_embedding = Dense(b_emb_dim, activation=activation, name=\"Band-Embedding\")(band_input)\n",
    "    \n",
    "    # Track embedding\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Dropout(rate=dropout)(x)\n",
    "    x = Conv1D(32, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(rate=dropout)(x)\n",
    "    seq_emb = Dense(8, activation='relu')(x)\n",
    "    \n",
    "    # Concatenate and create product\n",
    "    concat = Concatenate(name=\"Concat\", axis=-1)([influencer_full_emb, band_embedding, seq_emb])\n",
    "    dense = Dense(last_dense, activation=activation, name=\"Dense\")(concat)\n",
    "    \n",
    "    # Dropout\n",
    "    dropout = Dropout(rate=dropout)(dense)\n",
    "    \n",
    "    # Output\n",
    "    output = Dense(1, activation=activation, name=\"Output\")(dropout)\n",
    "    band_embedding_model = Model([influencer_input, band_input, influencer_kind_input, sequence_input],\n",
    "                                 band_embedding)\n",
    "    influencer_embedding_model = Model([influencer_input, band_input, influencer_kind_input, sequence_input],\n",
    "                                       influencer_full_emb)\n",
    "    model = Model([influencer_input, band_input, influencer_kind_input, sequence_input], output)\n",
    "    model.compile('adam', 'mean_squared_error')\n",
    "    \n",
    "    return model, band_embedding_model, influencer_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60267 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60267/60267 [==============================] - 47s 783us/step - loss: 0.1275 - val_loss: 0.1073\n",
      "Epoch 2/200\n",
      "60267/60267 [==============================] - 47s 778us/step - loss: 0.1085 - val_loss: 0.1048\n",
      "Epoch 3/200\n",
      "60267/60267 [==============================] - 45s 749us/step - loss: 0.1054 - val_loss: 0.0996\n",
      "Epoch 4/200\n",
      "60267/60267 [==============================] - 44s 729us/step - loss: 0.1025 - val_loss: 0.1001\n",
      "Epoch 5/200\n",
      "60267/60267 [==============================] - 44s 723us/step - loss: 0.1011 - val_loss: 0.0965\n",
      "Epoch 6/200\n",
      "60267/60267 [==============================] - 44s 732us/step - loss: 0.1005 - val_loss: 0.0973\n",
      "Epoch 7/200\n",
      "60267/60267 [==============================] - 44s 733us/step - loss: 0.0996 - val_loss: 0.0966\n",
      "Epoch 8/200\n",
      "60267/60267 [==============================] - 46s 756us/step - loss: 0.0992 - val_loss: 0.0978\n",
      "Epoch 9/200\n",
      "60267/60267 [==============================] - 46s 767us/step - loss: 0.0987 - val_loss: 0.0981\n",
      "Epoch 10/200\n",
      "60267/60267 [==============================] - 45s 744us/step - loss: 0.0984 - val_loss: 0.0965\n",
      "Epoch 11/200\n",
      "60267/60267 [==============================] - 43s 714us/step - loss: 0.0967 - val_loss: 0.0954\n",
      "Epoch 12/200\n",
      "60267/60267 [==============================] - 44s 733us/step - loss: 0.0962 - val_loss: 0.0944\n",
      "Epoch 13/200\n",
      "60267/60267 [==============================] - 45s 749us/step - loss: 0.0958 - val_loss: 0.0945\n",
      "Epoch 14/200\n",
      "60267/60267 [==============================] - 44s 722us/step - loss: 0.0954 - val_loss: 0.0942\n",
      "Epoch 15/200\n",
      "60267/60267 [==============================] - 43s 708us/step - loss: 0.0953 - val_loss: 0.0945\n",
      "Epoch 16/200\n",
      "60267/60267 [==============================] - 44s 729us/step - loss: 0.0955 - val_loss: 0.0946\n",
      "Epoch 17/200\n",
      "60267/60267 [==============================] - 45s 752us/step - loss: 0.0951 - val_loss: 0.0941\n",
      "Epoch 18/200\n",
      "60267/60267 [==============================] - 46s 758us/step - loss: 0.0951 - val_loss: 0.0959\n",
      "Epoch 19/200\n",
      "60267/60267 [==============================] - 45s 742us/step - loss: 0.0947 - val_loss: 0.0948\n",
      "Epoch 20/200\n",
      "60267/60267 [==============================] - 43s 708us/step - loss: 0.0943 - val_loss: 0.0940\n",
      "Epoch 21/200\n",
      "60267/60267 [==============================] - 42s 705us/step - loss: 0.0943 - val_loss: 0.0943\n",
      "Epoch 22/200\n",
      "60267/60267 [==============================] - 43s 719us/step - loss: 0.0940 - val_loss: 0.0952\n",
      "Epoch 23/200\n",
      "60267/60267 [==============================] - 45s 745us/step - loss: 0.0942 - val_loss: 0.0941\n",
      "Epoch 24/200\n",
      "60267/60267 [==============================] - 45s 738us/step - loss: 0.0941 - val_loss: 0.0938\n",
      "Epoch 25/200\n",
      "60267/60267 [==============================] - 44s 738us/step - loss: 0.0936 - val_loss: 0.0940\n",
      "Epoch 26/200\n",
      "60267/60267 [==============================] - 44s 734us/step - loss: 0.0938 - val_loss: 0.0936\n",
      "Epoch 27/200\n",
      "60267/60267 [==============================] - 43s 709us/step - loss: 0.0935 - val_loss: 0.0938\n",
      "Epoch 28/200\n",
      "60267/60267 [==============================] - 44s 734us/step - loss: 0.0933 - val_loss: 0.0953\n",
      "Epoch 29/200\n",
      "60267/60267 [==============================] - 44s 729us/step - loss: 0.0932 - val_loss: 0.0937\n",
      "Epoch 30/200\n",
      "60267/60267 [==============================] - 45s 740us/step - loss: 0.0935 - val_loss: 0.0936\n",
      "Epoch 31/200\n",
      "60267/60267 [==============================] - 43s 710us/step - loss: 0.0933 - val_loss: 0.0934\n",
      "Epoch 32/200\n",
      "60267/60267 [==============================] - 43s 710us/step - loss: 0.0930 - val_loss: 0.0934\n",
      "Epoch 33/200\n",
      "60267/60267 [==============================] - 43s 712us/step - loss: 0.0926 - val_loss: 0.0940\n",
      "Epoch 34/200\n",
      "60267/60267 [==============================] - 43s 712us/step - loss: 0.0923 - val_loss: 0.0949\n",
      "Epoch 35/200\n",
      "60267/60267 [==============================] - 43s 713us/step - loss: 0.0923 - val_loss: 0.0931\n",
      "Epoch 36/200\n",
      "60267/60267 [==============================] - 43s 711us/step - loss: 0.0925 - val_loss: 0.0942\n",
      "Epoch 37/200\n",
      "60267/60267 [==============================] - 43s 710us/step - loss: 0.0927 - val_loss: 0.0937\n",
      "Epoch 38/200\n",
      "60267/60267 [==============================] - 43s 713us/step - loss: 0.0926 - val_loss: 0.0934\n",
      "Epoch 39/200\n",
      "60267/60267 [==============================] - 43s 713us/step - loss: 0.0922 - val_loss: 0.0938\n",
      "Epoch 40/200\n",
      "60267/60267 [==============================] - 43s 714us/step - loss: 0.0919 - val_loss: 0.0944\n",
      "Epoch 41/200\n",
      "60267/60267 [==============================] - 43s 717us/step - loss: 0.0912 - val_loss: 0.0934\n",
      "Epoch 42/200\n",
      "60267/60267 [==============================] - 43s 711us/step - loss: 0.0906 - val_loss: 0.0931\n",
      "Epoch 43/200\n",
      "60267/60267 [==============================] - 43s 712us/step - loss: 0.0904 - val_loss: 0.0931\n",
      "Epoch 44/200\n",
      "60267/60267 [==============================] - 43s 720us/step - loss: 0.0903 - val_loss: 0.0931\n",
      "Epoch 45/200\n",
      "60267/60267 [==============================] - 44s 726us/step - loss: 0.0902 - val_loss: 0.0931\n",
      "Epoch 46/200\n",
      "60267/60267 [==============================] - 44s 728us/step - loss: 0.0895 - val_loss: 0.0928\n",
      "Epoch 47/200\n",
      "60267/60267 [==============================] - 43s 715us/step - loss: 0.0893 - val_loss: 0.0926\n",
      "Epoch 48/200\n",
      "60267/60267 [==============================] - 43s 709us/step - loss: 0.0896 - val_loss: 0.0927\n",
      "Epoch 49/200\n",
      "60267/60267 [==============================] - 42s 704us/step - loss: 0.0889 - val_loss: 0.0925\n",
      "Epoch 50/200\n",
      "60267/60267 [==============================] - 42s 703us/step - loss: 0.0892 - val_loss: 0.0923\n",
      "Epoch 51/200\n",
      "60267/60267 [==============================] - 42s 703us/step - loss: 0.0890 - val_loss: 0.0926\n",
      "Epoch 52/200\n",
      "60267/60267 [==============================] - 42s 704us/step - loss: 0.0886 - val_loss: 0.0923\n",
      "Epoch 53/200\n",
      "60267/60267 [==============================] - 44s 738us/step - loss: 0.0885 - val_loss: 0.0924\n",
      "Epoch 54/200\n",
      "60267/60267 [==============================] - 44s 726us/step - loss: 0.0888 - val_loss: 0.0927\n",
      "Epoch 55/200\n",
      "60267/60267 [==============================] - 43s 720us/step - loss: 0.0882 - val_loss: 0.0925\n",
      "Epoch 56/200\n",
      "60267/60267 [==============================] - 46s 763us/step - loss: 0.0879 - val_loss: 0.0925\n",
      "Epoch 57/200\n",
      "60267/60267 [==============================] - 43s 714us/step - loss: 0.0880 - val_loss: 0.0924\n",
      "Epoch 58/200\n",
      "60267/60267 [==============================] - 43s 715us/step - loss: 0.0878 - val_loss: 0.0923\n",
      "Epoch 59/200\n",
      "60267/60267 [==============================] - 43s 716us/step - loss: 0.0880 - val_loss: 0.0924\n",
      "Epoch 60/200\n",
      "60267/60267 [==============================] - 44s 723us/step - loss: 0.0877 - val_loss: 0.0923\n",
      "Epoch 61/200\n",
      "60267/60267 [==============================] - 43s 717us/step - loss: 0.0877 - val_loss: 0.0923\n",
      "Epoch 62/200\n",
      "60267/60267 [==============================] - 43s 715us/step - loss: 0.0876 - val_loss: 0.0923\n",
      "Epoch 63/200\n",
      "60267/60267 [==============================] - 43s 718us/step - loss: 0.0875 - val_loss: 0.0922\n",
      "Epoch 64/200\n",
      "60267/60267 [==============================] - 43s 720us/step - loss: 0.0879 - val_loss: 0.0923\n",
      "Epoch 65/200\n",
      "60267/60267 [==============================] - 43s 715us/step - loss: 0.0875 - val_loss: 0.0923\n",
      "Epoch 66/200\n",
      "60267/60267 [==============================] - 43s 715us/step - loss: 0.0873 - val_loss: 0.0923\n",
      "Epoch 67/200\n",
      "60267/60267 [==============================] - 43s 720us/step - loss: 0.0871 - val_loss: 0.0924\n",
      "Epoch 68/200\n",
      "60267/60267 [==============================] - 43s 717us/step - loss: 0.0877 - val_loss: 0.0922\n",
      "Epoch 69/200\n",
      "60267/60267 [==============================] - 43s 711us/step - loss: 0.0871 - val_loss: 0.0923\n",
      "Epoch 70/200\n",
      "60267/60267 [==============================] - 43s 709us/step - loss: 0.0876 - val_loss: 0.0923\n",
      "Epoch 71/200\n",
      "60267/60267 [==============================] - 43s 713us/step - loss: 0.0874 - val_loss: 0.0923\n",
      "Epoch 72/200\n",
      "60267/60267 [==============================] - 43s 707us/step - loss: 0.0873 - val_loss: 0.0923\n",
      "Epoch 73/200\n",
      "60267/60267 [==============================] - 43s 713us/step - loss: 0.0875 - val_loss: 0.0924\n",
      "Epoch 74/200\n",
      "60267/60267 [==============================] - 43s 706us/step - loss: 0.0868 - val_loss: 0.0923\n",
      "Epoch 75/200\n",
      "60267/60267 [==============================] - 43s 714us/step - loss: 0.0872 - val_loss: 0.0923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "60267/60267 [==============================] - 45s 753us/step - loss: 0.0872 - val_loss: 0.0923\n",
      "Epoch 77/200\n",
      "60267/60267 [==============================] - 45s 750us/step - loss: 0.0868 - val_loss: 0.0924\n",
      "Epoch 78/200\n",
      "60267/60267 [==============================] - 46s 756us/step - loss: 0.0870 - val_loss: 0.0923\n",
      "Epoch 79/200\n",
      "60267/60267 [==============================] - 46s 757us/step - loss: 0.0875 - val_loss: 0.0923\n",
      "Epoch 80/200\n",
      "60267/60267 [==============================] - 46s 757us/step - loss: 0.0874 - val_loss: 0.0924\n",
      "Epoch 81/200\n",
      "60267/60267 [==============================] - 46s 758us/step - loss: 0.0871 - val_loss: 0.0923\n",
      "Epoch 82/200\n",
      "60267/60267 [==============================] - 45s 753us/step - loss: 0.0870 - val_loss: 0.0923\n",
      "Epoch 83/200\n",
      "60267/60267 [==============================] - 46s 758us/step - loss: 0.0868 - val_loss: 0.0924\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00083: early stopping\n",
      "Train on 60268 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.1284 - val_loss: 0.1078\n",
      "Epoch 2/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.1099 - val_loss: 0.1036\n",
      "Epoch 3/200\n",
      "60268/60268 [==============================] - 45s 742us/step - loss: 0.1060 - val_loss: 0.0994\n",
      "Epoch 4/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.1038 - val_loss: 0.1004\n",
      "Epoch 5/200\n",
      "60268/60268 [==============================] - 45s 744us/step - loss: 0.1020 - val_loss: 0.1001\n",
      "Epoch 6/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.1008 - val_loss: 0.0986\n",
      "Epoch 7/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.1002 - val_loss: 0.0969\n",
      "Epoch 8/200\n",
      "60268/60268 [==============================] - 44s 738us/step - loss: 0.0995 - val_loss: 0.0971\n",
      "Epoch 9/200\n",
      "60268/60268 [==============================] - 45s 747us/step - loss: 0.0993 - val_loss: 0.0983\n",
      "Epoch 10/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0983 - val_loss: 0.0967\n",
      "Epoch 11/200\n",
      "60268/60268 [==============================] - 45s 741us/step - loss: 0.0982 - val_loss: 0.0967\n",
      "Epoch 12/200\n",
      "60268/60268 [==============================] - 45s 745us/step - loss: 0.0977 - val_loss: 0.0984\n",
      "Epoch 13/200\n",
      "60268/60268 [==============================] - 45s 745us/step - loss: 0.0974 - val_loss: 0.0965\n",
      "Epoch 14/200\n",
      "60268/60268 [==============================] - 45s 742us/step - loss: 0.0971 - val_loss: 0.0962\n",
      "Epoch 15/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0970 - val_loss: 0.0970\n",
      "Epoch 16/200\n",
      "60268/60268 [==============================] - 45s 747us/step - loss: 0.0966 - val_loss: 0.0967\n",
      "Epoch 17/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0965 - val_loss: 0.0959\n",
      "Epoch 18/200\n",
      "60268/60268 [==============================] - 45s 748us/step - loss: 0.0964 - val_loss: 0.0963\n",
      "Epoch 19/200\n",
      "60268/60268 [==============================] - 45s 739us/step - loss: 0.0959 - val_loss: 0.0981\n",
      "Epoch 20/200\n",
      "60268/60268 [==============================] - 45s 747us/step - loss: 0.0961 - val_loss: 0.0959\n",
      "Epoch 21/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0959 - val_loss: 0.0957\n",
      "Epoch 22/200\n",
      "60268/60268 [==============================] - 45s 745us/step - loss: 0.0956 - val_loss: 0.0971\n",
      "Epoch 23/200\n",
      "60268/60268 [==============================] - 45s 747us/step - loss: 0.0956 - val_loss: 0.0962\n",
      "Epoch 24/200\n",
      "60268/60268 [==============================] - 45s 741us/step - loss: 0.0949 - val_loss: 0.0960\n",
      "Epoch 25/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0951 - val_loss: 0.0953\n",
      "Epoch 26/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0949 - val_loss: 0.0971\n",
      "Epoch 27/200\n",
      "60268/60268 [==============================] - 45s 739us/step - loss: 0.0951 - val_loss: 0.0955\n",
      "Epoch 28/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0951 - val_loss: 0.0966\n",
      "Epoch 29/200\n",
      "60268/60268 [==============================] - 45s 742us/step - loss: 0.0949 - val_loss: 0.0966\n",
      "Epoch 30/200\n",
      "60268/60268 [==============================] - 45s 740us/step - loss: 0.0949 - val_loss: 0.0955\n",
      "Epoch 31/200\n",
      "60268/60268 [==============================] - 45s 741us/step - loss: 0.0934 - val_loss: 0.0953\n",
      "Epoch 32/200\n",
      "60268/60268 [==============================] - 45s 742us/step - loss: 0.0931 - val_loss: 0.0962\n",
      "Epoch 33/200\n",
      "60268/60268 [==============================] - 45s 739us/step - loss: 0.0931 - val_loss: 0.0948\n",
      "Epoch 34/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0927 - val_loss: 0.0951\n",
      "Epoch 35/200\n",
      "60268/60268 [==============================] - 45s 742us/step - loss: 0.0927 - val_loss: 0.0951\n",
      "Epoch 36/200\n",
      "60268/60268 [==============================] - 45s 744us/step - loss: 0.0923 - val_loss: 0.0946\n",
      "Epoch 37/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0925 - val_loss: 0.0960\n",
      "Epoch 38/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0924 - val_loss: 0.0953\n",
      "Epoch 39/200\n",
      "60268/60268 [==============================] - 45s 741us/step - loss: 0.0923 - val_loss: 0.0956\n",
      "Epoch 40/200\n",
      "60268/60268 [==============================] - 45s 745us/step - loss: 0.0922 - val_loss: 0.0951\n",
      "Epoch 41/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0919 - val_loss: 0.0951\n",
      "Epoch 42/200\n",
      "60268/60268 [==============================] - 45s 744us/step - loss: 0.0914 - val_loss: 0.0950\n",
      "Epoch 43/200\n",
      "60268/60268 [==============================] - 45s 744us/step - loss: 0.0911 - val_loss: 0.0947\n",
      "Epoch 44/200\n",
      "60268/60268 [==============================] - 44s 737us/step - loss: 0.0912 - val_loss: 0.0949\n",
      "Epoch 45/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0912 - val_loss: 0.0946\n",
      "Epoch 46/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0910 - val_loss: 0.0951\n",
      "Epoch 47/200\n",
      "60268/60268 [==============================] - 45s 744us/step - loss: 0.0906 - val_loss: 0.0948\n",
      "Epoch 48/200\n",
      "60268/60268 [==============================] - 44s 737us/step - loss: 0.0906 - val_loss: 0.0947\n",
      "Epoch 49/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0904 - val_loss: 0.0948\n",
      "Epoch 50/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0903 - val_loss: 0.0949\n",
      "Epoch 51/200\n",
      "60268/60268 [==============================] - 45s 739us/step - loss: 0.0902 - val_loss: 0.0948\n",
      "Epoch 52/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0900 - val_loss: 0.0946\n",
      "Epoch 53/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0899 - val_loss: 0.0947\n",
      "Epoch 54/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0898 - val_loss: 0.0947\n",
      "Epoch 55/200\n",
      "60268/60268 [==============================] - 44s 738us/step - loss: 0.0896 - val_loss: 0.0947\n",
      "Epoch 56/200\n",
      "60268/60268 [==============================] - 44s 738us/step - loss: 0.0901 - val_loss: 0.0946\n",
      "Epoch 57/200\n",
      "60268/60268 [==============================] - 44s 737us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 58/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0896 - val_loss: 0.0946\n",
      "Epoch 59/200\n",
      "60268/60268 [==============================] - 45s 748us/step - loss: 0.0899 - val_loss: 0.0947\n",
      "Epoch 60/200\n",
      "60268/60268 [==============================] - 45s 740us/step - loss: 0.0897 - val_loss: 0.0947\n",
      "Epoch 61/200\n",
      "60268/60268 [==============================] - 45s 741us/step - loss: 0.0896 - val_loss: 0.0945\n",
      "Epoch 62/200\n",
      "60268/60268 [==============================] - 44s 738us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 63/200\n",
      "60268/60268 [==============================] - 44s 735us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 64/200\n",
      "60268/60268 [==============================] - 44s 737us/step - loss: 0.0896 - val_loss: 0.0945\n",
      "Epoch 65/200\n",
      "60268/60268 [==============================] - 45s 739us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 66/200\n",
      "60268/60268 [==============================] - 44s 736us/step - loss: 0.0893 - val_loss: 0.0945\n",
      "Epoch 67/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0893 - val_loss: 0.0946\n",
      "Epoch 68/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0895 - val_loss: 0.0946\n",
      "Epoch 69/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 70/200\n",
      "60268/60268 [==============================] - 46s 756us/step - loss: 0.0895 - val_loss: 0.0946\n",
      "Epoch 71/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 72/200\n",
      "60268/60268 [==============================] - 46s 766us/step - loss: 0.0893 - val_loss: 0.0946\n",
      "Epoch 73/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0890 - val_loss: 0.0946\n",
      "Epoch 74/200\n",
      "60268/60268 [==============================] - 45s 745us/step - loss: 0.0893 - val_loss: 0.0946\n",
      "Epoch 75/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 76/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 77/200\n",
      "60268/60268 [==============================] - 46s 756us/step - loss: 0.0893 - val_loss: 0.0946\n",
      "Epoch 78/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0890 - val_loss: 0.0946\n",
      "Epoch 79/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0890 - val_loss: 0.0946\n",
      "Epoch 80/200\n",
      "60268/60268 [==============================] - 45s 749us/step - loss: 0.0895 - val_loss: 0.0946\n",
      "Epoch 81/200\n",
      "60268/60268 [==============================] - 45s 748us/step - loss: 0.0892 - val_loss: 0.0946\n",
      "Epoch 82/200\n",
      "60268/60268 [==============================] - 45s 745us/step - loss: 0.0892 - val_loss: 0.0946\n",
      "Epoch 83/200\n",
      "60268/60268 [==============================] - 45s 745us/step - loss: 0.0893 - val_loss: 0.0946\n",
      "Epoch 84/200\n",
      "60268/60268 [==============================] - 45s 748us/step - loss: 0.0894 - val_loss: 0.0946\n",
      "Epoch 85/200\n",
      "60268/60268 [==============================] - 45s 747us/step - loss: 0.0893 - val_loss: 0.0946\n",
      "Epoch 86/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0893 - val_loss: 0.0946\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00086: early stopping\n",
      "Train on 60268 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60268/60268 [==============================] - 48s 801us/step - loss: 0.1294 - val_loss: 0.1048\n",
      "Epoch 2/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.1083 - val_loss: 0.1000\n",
      "Epoch 3/200\n",
      "60268/60268 [==============================] - 45s 748us/step - loss: 0.1042 - val_loss: 0.0971\n",
      "Epoch 4/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.1027 - val_loss: 0.0953\n",
      "Epoch 5/200\n",
      "60268/60268 [==============================] - 45s 745us/step - loss: 0.1014 - val_loss: 0.0981\n",
      "Epoch 6/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.1002 - val_loss: 0.0964\n",
      "Epoch 7/200\n",
      "60268/60268 [==============================] - 46s 764us/step - loss: 0.0991 - val_loss: 0.0942\n",
      "Epoch 8/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0984 - val_loss: 0.0957\n",
      "Epoch 9/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0985 - val_loss: 0.0942\n",
      "Epoch 10/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0978 - val_loss: 0.0947\n",
      "Epoch 11/200\n",
      "60268/60268 [==============================] - 46s 756us/step - loss: 0.0973 - val_loss: 0.0956\n",
      "Epoch 12/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0967 - val_loss: 0.0945\n",
      "Epoch 13/200\n",
      "60268/60268 [==============================] - 46s 755us/step - loss: 0.0951 - val_loss: 0.0935\n",
      "Epoch 14/200\n",
      "60268/60268 [==============================] - 46s 755us/step - loss: 0.0944 - val_loss: 0.0943\n",
      "Epoch 15/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0945 - val_loss: 0.0939\n",
      "Epoch 16/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0941 - val_loss: 0.0939\n",
      "Epoch 17/200\n",
      "60268/60268 [==============================] - 46s 762us/step - loss: 0.0937 - val_loss: 0.0933\n",
      "Epoch 18/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0936 - val_loss: 0.0933\n",
      "Epoch 19/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0933 - val_loss: 0.0933\n",
      "Epoch 20/200\n",
      "60268/60268 [==============================] - 45s 744us/step - loss: 0.0931 - val_loss: 0.0936\n",
      "Epoch 21/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0932 - val_loss: 0.0929\n",
      "Epoch 22/200\n",
      "60268/60268 [==============================] - 46s 758us/step - loss: 0.0926 - val_loss: 0.0939\n",
      "Epoch 23/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0926 - val_loss: 0.0935\n",
      "Epoch 24/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0925 - val_loss: 0.0937\n",
      "Epoch 25/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0920 - val_loss: 0.0932\n",
      "Epoch 26/200\n",
      "60268/60268 [==============================] - 46s 756us/step - loss: 0.0923 - val_loss: 0.0933\n",
      "Epoch 27/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0908 - val_loss: 0.0929\n",
      "Epoch 28/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0908 - val_loss: 0.0930\n",
      "Epoch 29/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0908 - val_loss: 0.0930\n",
      "Epoch 30/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0907 - val_loss: 0.0930\n",
      "Epoch 31/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0901 - val_loss: 0.0934\n",
      "Epoch 32/200\n",
      "60268/60268 [==============================] - 45s 743us/step - loss: 0.0898 - val_loss: 0.0932\n",
      "Epoch 33/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0897 - val_loss: 0.0929\n",
      "Epoch 34/200\n",
      "60268/60268 [==============================] - 45s 749us/step - loss: 0.0896 - val_loss: 0.0928\n",
      "Epoch 35/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0894 - val_loss: 0.0928\n",
      "Epoch 36/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0889 - val_loss: 0.0927\n",
      "Epoch 37/200\n",
      "60268/60268 [==============================] - 45s 749us/step - loss: 0.0893 - val_loss: 0.0932\n",
      "Epoch 38/200\n",
      "60268/60268 [==============================] - 45s 747us/step - loss: 0.0891 - val_loss: 0.0931\n",
      "Epoch 39/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0892 - val_loss: 0.0930\n",
      "Epoch 40/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0888 - val_loss: 0.0929\n",
      "Epoch 41/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0886 - val_loss: 0.0928\n",
      "Epoch 42/200\n",
      "60268/60268 [==============================] - 45s 746us/step - loss: 0.0885 - val_loss: 0.0930\n",
      "Epoch 43/200\n",
      "60268/60268 [==============================] - 45s 744us/step - loss: 0.0884 - val_loss: 0.0928\n",
      "Epoch 44/200\n",
      "60268/60268 [==============================] - 45s 751us/step - loss: 0.0886 - val_loss: 0.0929\n",
      "Epoch 45/200\n",
      "60268/60268 [==============================] - 46s 758us/step - loss: 0.0882 - val_loss: 0.0928\n",
      "Epoch 46/200\n",
      "60268/60268 [==============================] - 45s 751us/step - loss: 0.0883 - val_loss: 0.0929\n",
      "Epoch 47/200\n",
      "60268/60268 [==============================] - 44s 736us/step - loss: 0.0878 - val_loss: 0.0929\n",
      "Epoch 48/200\n",
      "60268/60268 [==============================] - 44s 730us/step - loss: 0.0879 - val_loss: 0.0929\n",
      "Epoch 49/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0881 - val_loss: 0.0928\n",
      "Epoch 50/200\n",
      "60268/60268 [==============================] - 45s 748us/step - loss: 0.0880 - val_loss: 0.0929\n",
      "Epoch 51/200\n",
      "60268/60268 [==============================] - 45s 751us/step - loss: 0.0875 - val_loss: 0.0929\n",
      "Epoch 52/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0877 - val_loss: 0.0929\n",
      "Epoch 53/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0878 - val_loss: 0.0929\n",
      "Epoch 54/200\n",
      "60268/60268 [==============================] - 45s 750us/step - loss: 0.0878 - val_loss: 0.0929\n",
      "Epoch 55/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0879 - val_loss: 0.0929\n",
      "Epoch 56/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0876 - val_loss: 0.0929\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00056: early stopping\n",
      "Train on 60268 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.1266 - val_loss: 0.1041\n",
      "Epoch 2/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.1084 - val_loss: 0.0980\n",
      "Epoch 3/200\n",
      "60268/60268 [==============================] - 46s 762us/step - loss: 0.1037 - val_loss: 0.0967\n",
      "Epoch 4/200\n",
      "60268/60268 [==============================] - 46s 761us/step - loss: 0.1020 - val_loss: 0.0960\n",
      "Epoch 5/200\n",
      "60268/60268 [==============================] - 46s 763us/step - loss: 0.1007 - val_loss: 0.0955\n",
      "Epoch 6/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0996 - val_loss: 0.0977\n",
      "Epoch 7/200\n",
      "60268/60268 [==============================] - 46s 762us/step - loss: 0.0988 - val_loss: 0.0951\n",
      "Epoch 8/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0986 - val_loss: 0.0949\n",
      "Epoch 9/200\n",
      "60268/60268 [==============================] - 46s 761us/step - loss: 0.0979 - val_loss: 0.0944\n",
      "Epoch 10/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0975 - val_loss: 0.0945\n",
      "Epoch 11/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0974 - val_loss: 0.0936\n",
      "Epoch 12/200\n",
      "60268/60268 [==============================] - 46s 768us/step - loss: 0.0968 - val_loss: 0.0949\n",
      "Epoch 13/200\n",
      "60268/60268 [==============================] - 46s 764us/step - loss: 0.0966 - val_loss: 0.0944\n",
      "Epoch 14/200\n",
      "60268/60268 [==============================] - 46s 769us/step - loss: 0.0961 - val_loss: 0.0951\n",
      "Epoch 15/200\n",
      "60268/60268 [==============================] - 46s 763us/step - loss: 0.0958 - val_loss: 0.0940\n",
      "Epoch 16/200\n",
      "60268/60268 [==============================] - 46s 769us/step - loss: 0.0956 - val_loss: 0.0941\n",
      "Epoch 17/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0939 - val_loss: 0.0936\n",
      "Epoch 18/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0934 - val_loss: 0.0939\n",
      "Epoch 19/200\n",
      "60268/60268 [==============================] - 46s 761us/step - loss: 0.0931 - val_loss: 0.0944\n",
      "Epoch 20/200\n",
      "60268/60268 [==============================] - 46s 756us/step - loss: 0.0931 - val_loss: 0.0934\n",
      "Epoch 21/200\n",
      "60268/60268 [==============================] - 46s 762us/step - loss: 0.0931 - val_loss: 0.0944\n",
      "Epoch 22/200\n",
      "60268/60268 [==============================] - 46s 763us/step - loss: 0.0929 - val_loss: 0.0942\n",
      "Epoch 23/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0925 - val_loss: 0.0937\n",
      "Epoch 24/200\n",
      "60268/60268 [==============================] - 46s 767us/step - loss: 0.0920 - val_loss: 0.0924\n",
      "Epoch 25/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0924 - val_loss: 0.0928\n",
      "Epoch 26/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0918 - val_loss: 0.0928\n",
      "Epoch 27/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0919 - val_loss: 0.0931\n",
      "Epoch 28/200\n",
      "60268/60268 [==============================] - 45s 751us/step - loss: 0.0920 - val_loss: 0.0929\n",
      "Epoch 29/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0912 - val_loss: 0.0931\n",
      "Epoch 30/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0904 - val_loss: 0.0930\n",
      "Epoch 31/200\n",
      "60268/60268 [==============================] - 45s 755us/step - loss: 0.0904 - val_loss: 0.0930\n",
      "Epoch 32/200\n",
      "60268/60268 [==============================] - 46s 763us/step - loss: 0.0898 - val_loss: 0.0933\n",
      "Epoch 33/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0898 - val_loss: 0.0932\n",
      "Epoch 34/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0895 - val_loss: 0.0939\n",
      "Epoch 35/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0891 - val_loss: 0.0935\n",
      "Epoch 36/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0890 - val_loss: 0.0934\n",
      "Epoch 37/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0890 - val_loss: 0.0934\n",
      "Epoch 38/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0890 - val_loss: 0.0934\n",
      "Epoch 39/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0889 - val_loss: 0.0935\n",
      "Epoch 40/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0883 - val_loss: 0.0934\n",
      "Epoch 41/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0883 - val_loss: 0.0933\n",
      "Epoch 42/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0883 - val_loss: 0.0934\n",
      "Epoch 43/200\n",
      "60268/60268 [==============================] - 45s 751us/step - loss: 0.0881 - val_loss: 0.0933\n",
      "Epoch 44/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0883 - val_loss: 0.0935\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00044: early stopping\n",
      "Train on 60268 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60268/60268 [==============================] - 48s 802us/step - loss: 0.1251 - val_loss: 0.1036\n",
      "Epoch 2/200\n",
      "60268/60268 [==============================] - 46s 763us/step - loss: 0.1063 - val_loss: 0.1004\n",
      "Epoch 3/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.1038 - val_loss: 0.0985\n",
      "Epoch 4/200\n",
      "60268/60268 [==============================] - 46s 758us/step - loss: 0.1014 - val_loss: 0.0967\n",
      "Epoch 5/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.1002 - val_loss: 0.0953\n",
      "Epoch 6/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0992 - val_loss: 0.0943\n",
      "Epoch 7/200\n",
      "60268/60268 [==============================] - 45s 748us/step - loss: 0.0982 - val_loss: 0.0938\n",
      "Epoch 8/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0970 - val_loss: 0.0947\n",
      "Epoch 9/200\n",
      "60268/60268 [==============================] - 46s 759us/step - loss: 0.0961 - val_loss: 0.0938\n",
      "Epoch 10/200\n",
      "60268/60268 [==============================] - 45s 747us/step - loss: 0.0954 - val_loss: 0.0948\n",
      "Epoch 11/200\n",
      "60268/60268 [==============================] - 46s 755us/step - loss: 0.0947 - val_loss: 0.0941\n",
      "Epoch 12/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0942 - val_loss: 0.0942\n",
      "Epoch 13/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0916 - val_loss: 0.0930\n",
      "Epoch 14/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0907 - val_loss: 0.0945\n",
      "Epoch 15/200\n",
      "60268/60268 [==============================] - 46s 756us/step - loss: 0.0902 - val_loss: 0.0933\n",
      "Epoch 16/200\n",
      "60268/60268 [==============================] - 46s 758us/step - loss: 0.0895 - val_loss: 0.0934\n",
      "Epoch 17/200\n",
      "60268/60268 [==============================] - 46s 762us/step - loss: 0.0899 - val_loss: 0.0929\n",
      "Epoch 18/200\n",
      "60268/60268 [==============================] - 46s 757us/step - loss: 0.0894 - val_loss: 0.0933\n",
      "Epoch 19/200\n",
      "60268/60268 [==============================] - 45s 751us/step - loss: 0.0879 - val_loss: 0.0929\n",
      "Epoch 20/200\n",
      "60268/60268 [==============================] - 45s 755us/step - loss: 0.0876 - val_loss: 0.0936\n",
      "Epoch 21/200\n",
      "60268/60268 [==============================] - 46s 756us/step - loss: 0.0869 - val_loss: 0.0932\n",
      "Epoch 22/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0870 - val_loss: 0.0935\n",
      "Epoch 23/200\n",
      "60268/60268 [==============================] - 45s 752us/step - loss: 0.0862 - val_loss: 0.0936\n",
      "Epoch 24/200\n",
      "60268/60268 [==============================] - 46s 756us/step - loss: 0.0858 - val_loss: 0.0934\n",
      "Epoch 25/200\n",
      "60268/60268 [==============================] - 45s 753us/step - loss: 0.0858 - val_loss: 0.0935\n",
      "Epoch 26/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0852 - val_loss: 0.0935\n",
      "Epoch 27/200\n",
      "60268/60268 [==============================] - 45s 754us/step - loss: 0.0853 - val_loss: 0.0938\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60268/60268 [==============================] - 46s 763us/step - loss: 0.0854 - val_loss: 0.0936\n",
      "Epoch 29/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0847 - val_loss: 0.0939\n",
      "Epoch 30/200\n",
      "60268/60268 [==============================] - 46s 767us/step - loss: 0.0846 - val_loss: 0.0938\n",
      "Epoch 31/200\n",
      "60268/60268 [==============================] - 46s 765us/step - loss: 0.0844 - val_loss: 0.0938\n",
      "Epoch 32/200\n",
      "60268/60268 [==============================] - 46s 761us/step - loss: 0.0843 - val_loss: 0.0939\n",
      "Epoch 33/200\n",
      "60268/60268 [==============================] - 46s 764us/step - loss: 0.0845 - val_loss: 0.0937\n",
      "Epoch 34/200\n",
      "60268/60268 [==============================] - 46s 763us/step - loss: 0.0840 - val_loss: 0.0939\n",
      "Epoch 35/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.0839 - val_loss: 0.0940\n",
      "Epoch 36/200\n",
      "60268/60268 [==============================] - 46s 766us/step - loss: 0.0840 - val_loss: 0.0942\n",
      "Epoch 37/200\n",
      "60268/60268 [==============================] - 46s 766us/step - loss: 0.0841 - val_loss: 0.0940\n",
      "Epoch 38/200\n",
      "60268/60268 [==============================] - 46s 763us/step - loss: 0.0841 - val_loss: 0.0942\n",
      "Epoch 39/200\n",
      "60268/60268 [==============================] - 46s 762us/step - loss: 0.0844 - val_loss: 0.0940\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00039: early stopping\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "PATIENCE = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "nn_score = 0.0\n",
    "\n",
    "for train_index, test_index in skf.split(X, (y * 100).astype(int)):\n",
    "    \n",
    "    X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "    y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "    \n",
    "    docs_train, docs_test = data[train_index], data[test_index]\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=RANDOM_SEED)\n",
    "    tridx, vidx = next(sss.split(X_train, (y_train * 100).astype(int)))\n",
    "    X_train, X_valid = X_train[tridx], X_train[vidx]\n",
    "    y_train, y_valid = y_train[tridx], y_train[vidx]\n",
    "    docs_train, docs_valid = docs_train[tridx], docs_train[vidx]\n",
    "    \n",
    "    # Build model\n",
    "    model, band_embedding_model, influencer_embedding_model = build_NN_model(dropout=0.2)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        mode='min', \n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                  patience=5)\n",
    "    \n",
    "    # Fit\n",
    "    model.fit([X_train[:, i_data_idx], X_train[:, b_data_idx], X_train[:, i_kind_idx], docs_train], \n",
    "              y_train,\n",
    "              validation_data=([X_valid[:, i_data_idx], \n",
    "                                X_valid[:, b_data_idx], \n",
    "                                X_valid[:, i_kind_idx],\n",
    "                                docs_valid], \n",
    "                               y_valid), \n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS,\n",
    "              callbacks=[es, reduce_lr],\n",
    "              verbose=1)\n",
    "    \n",
    "    nn_score += np.sqrt(mean_squared_error(\n",
    "        model.predict([X_test[:, i_data_idx], X_test[:, b_data_idx], X_test[:, i_kind_idx], docs_test]), \n",
    "        y_test\n",
    "    ))\n",
    "\n",
    "nn_score /= N_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3075894541807684\n"
     ]
    }
   ],
   "source": [
    "print(nn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NN_model(\n",
    "    i_emb_dim=16, \n",
    "    b_emb_dim=16, \n",
    "    kind_emb_dim=8, \n",
    "    last_dense=16, \n",
    "    dropout=0.2,\n",
    "    activation='relu'\n",
    "):\n",
    "    \"\"\"\n",
    "    Build simple MLP Neural Network.\n",
    "    \"\"\"\n",
    "    # Influencer embedding\n",
    "    influencer_input = Input(shape=[influencer_data.shape[1]], name=\"Influencer-Input\")\n",
    "    influencer_embedding = Dense(i_emb_dim, activation=activation, name=\"Influencer-Embedding\")(influencer_input)\n",
    "    \n",
    "    # Influencer kind categorical embedding\n",
    "    influencer_kind_input = Input(shape=[1], name=\"Influencer-Kind-Input\")\n",
    "    influencer_kind_emb = Embedding(\n",
    "        influencer_kind.nunique(), \n",
    "        kind_emb_dim, \n",
    "        name=\"Influencer-Kind-Embedding\"\n",
    "    )(influencer_kind_input)\n",
    "    \n",
    "    # Concatenate influencer emb with influencer kind emb to get full influencer emb\n",
    "    influencer_full_emb = Concatenate(name=\"Influencer-Full-Embedding\", axis=-1)(\n",
    "        [influencer_embedding, Flatten(name='Flatten')(influencer_kind_emb)]\n",
    "    )\n",
    "    \n",
    "    # Band embedding\n",
    "    band_input = Input(shape=[band_data.shape[1]], name=\"Band-Input\")\n",
    "    band_embedding = Dense(b_emb_dim, activation=activation, name=\"Band-Embedding\")(band_input)\n",
    "    \n",
    "    # Track embedding\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(64, 5, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Dropout(rate=dropout)(x)\n",
    "    x = Conv1D(32, 5, activation='relu')(x)\n",
    "    x = Dropout(rate=dropout)(x)\n",
    "    x = Conv1D(32, 5, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(rate=dropout)(x)\n",
    "    seq_emb = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    # Concatenate and create product\n",
    "    concat = Concatenate(name=\"Concat\", axis=-1)([influencer_full_emb, band_embedding, seq_emb])\n",
    "    dense = Dense(last_dense, activation=activation, name=\"Dense\")(concat)\n",
    "    \n",
    "    # Dropout\n",
    "    dropout = Dropout(rate=dropout)(dense)\n",
    "    \n",
    "    # Output\n",
    "    output = Dense(1, activation=activation, name=\"Output\")(dropout)\n",
    "    band_embedding_model = Model([influencer_input, band_input, influencer_kind_input, sequence_input],\n",
    "                                 band_embedding)\n",
    "    influencer_embedding_model = Model([influencer_input, band_input, influencer_kind_input, sequence_input],\n",
    "                                       influencer_full_emb)\n",
    "    model = Model([influencer_input, band_input, influencer_kind_input, sequence_input], output)\n",
    "    model.compile('adam', 'mean_squared_error')\n",
    "    \n",
    "    return model, band_embedding_model, influencer_embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60267 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60267/60267 [==============================] - 51s 841us/step - loss: 0.1276 - val_loss: 0.1095\n",
      "Epoch 2/200\n",
      "60267/60267 [==============================] - 47s 782us/step - loss: 0.1123 - val_loss: 0.1033\n",
      "Epoch 3/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.1082 - val_loss: 0.1004\n",
      "Epoch 4/200\n",
      "60267/60267 [==============================] - 47s 780us/step - loss: 0.1070 - val_loss: 0.0983\n",
      "Epoch 5/200\n",
      "60267/60267 [==============================] - 47s 787us/step - loss: 0.1054 - val_loss: 0.0980\n",
      "Epoch 6/200\n",
      "60267/60267 [==============================] - 47s 784us/step - loss: 0.1037 - val_loss: 0.0973\n",
      "Epoch 7/200\n",
      "60267/60267 [==============================] - 47s 780us/step - loss: 0.1034 - val_loss: 0.0964\n",
      "Epoch 8/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.1020 - val_loss: 0.0952\n",
      "Epoch 9/200\n",
      "60267/60267 [==============================] - 47s 783us/step - loss: 0.1017 - val_loss: 0.0958\n",
      "Epoch 10/200\n",
      "60267/60267 [==============================] - 47s 776us/step - loss: 0.1010 - val_loss: 0.0965\n",
      "Epoch 11/200\n",
      "60267/60267 [==============================] - 47s 779us/step - loss: 0.1008 - val_loss: 0.0950\n",
      "Epoch 12/200\n",
      "60267/60267 [==============================] - 47s 779us/step - loss: 0.1003 - val_loss: 0.0964\n",
      "Epoch 13/200\n",
      "60267/60267 [==============================] - 47s 781us/step - loss: 0.1003 - val_loss: 0.0957\n",
      "Epoch 14/200\n",
      "60267/60267 [==============================] - 47s 783us/step - loss: 0.0998 - val_loss: 0.0941\n",
      "Epoch 15/200\n",
      "60267/60267 [==============================] - 47s 777us/step - loss: 0.0997 - val_loss: 0.0945\n",
      "Epoch 16/200\n",
      "60267/60267 [==============================] - 47s 777us/step - loss: 0.0995 - val_loss: 0.0963\n",
      "Epoch 17/200\n",
      "60267/60267 [==============================] - 47s 780us/step - loss: 0.0990 - val_loss: 0.0939\n",
      "Epoch 18/200\n",
      "60267/60267 [==============================] - 47s 780us/step - loss: 0.0987 - val_loss: 0.0942\n",
      "Epoch 19/200\n",
      "60267/60267 [==============================] - 47s 779us/step - loss: 0.0985 - val_loss: 0.0955\n",
      "Epoch 20/200\n",
      "60267/60267 [==============================] - 47s 777us/step - loss: 0.0981 - val_loss: 0.0950\n",
      "Epoch 21/200\n",
      "60267/60267 [==============================] - 47s 779us/step - loss: 0.0978 - val_loss: 0.0938\n",
      "Epoch 22/200\n",
      "60267/60267 [==============================] - 47s 780us/step - loss: 0.0976 - val_loss: 0.0938\n",
      "Epoch 23/200\n",
      "60267/60267 [==============================] - 46s 770us/step - loss: 0.0971 - val_loss: 0.0933\n",
      "Epoch 24/200\n",
      "60267/60267 [==============================] - 47s 777us/step - loss: 0.0972 - val_loss: 0.0933\n",
      "Epoch 25/200\n",
      "60267/60267 [==============================] - 47s 782us/step - loss: 0.0970 - val_loss: 0.0927\n",
      "Epoch 26/200\n",
      "60267/60267 [==============================] - 47s 773us/step - loss: 0.0968 - val_loss: 0.0937\n",
      "Epoch 27/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.0966 - val_loss: 0.0940\n",
      "Epoch 28/200\n",
      "60267/60267 [==============================] - 47s 777us/step - loss: 0.0964 - val_loss: 0.0935\n",
      "Epoch 29/200\n",
      "60267/60267 [==============================] - 47s 777us/step - loss: 0.0963 - val_loss: 0.0929\n",
      "Epoch 30/200\n",
      "60267/60267 [==============================] - 47s 782us/step - loss: 0.0957 - val_loss: 0.0939\n",
      "Epoch 31/200\n",
      "60267/60267 [==============================] - 47s 781us/step - loss: 0.0947 - val_loss: 0.0934\n",
      "Epoch 32/200\n",
      "60267/60267 [==============================] - 47s 782us/step - loss: 0.0940 - val_loss: 0.0926\n",
      "Epoch 33/200\n",
      "60267/60267 [==============================] - 46s 768us/step - loss: 0.0939 - val_loss: 0.0934\n",
      "Epoch 34/200\n",
      "60267/60267 [==============================] - 47s 774us/step - loss: 0.0934 - val_loss: 0.0925\n",
      "Epoch 35/200\n",
      "60267/60267 [==============================] - 46s 771us/step - loss: 0.0939 - val_loss: 0.0927\n",
      "Epoch 36/200\n",
      "60267/60267 [==============================] - 47s 772us/step - loss: 0.0938 - val_loss: 0.0945\n",
      "Epoch 37/200\n",
      "60267/60267 [==============================] - 47s 777us/step - loss: 0.0935 - val_loss: 0.0940\n",
      "Epoch 38/200\n",
      "60267/60267 [==============================] - 46s 771us/step - loss: 0.0931 - val_loss: 0.0927\n",
      "Epoch 39/200\n",
      "60267/60267 [==============================] - 47s 783us/step - loss: 0.0929 - val_loss: 0.0921\n",
      "Epoch 40/200\n",
      "60267/60267 [==============================] - 47s 777us/step - loss: 0.0929 - val_loss: 0.0927\n",
      "Epoch 41/200\n",
      "60267/60267 [==============================] - 47s 772us/step - loss: 0.0927 - val_loss: 0.0928\n",
      "Epoch 42/200\n",
      "60267/60267 [==============================] - 47s 780us/step - loss: 0.0929 - val_loss: 0.0928\n",
      "Epoch 43/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.0928 - val_loss: 0.0926\n",
      "Epoch 44/200\n",
      "60267/60267 [==============================] - 47s 774us/step - loss: 0.0928 - val_loss: 0.0927\n",
      "Epoch 45/200\n",
      "60267/60267 [==============================] - 47s 773us/step - loss: 0.0919 - val_loss: 0.0922\n",
      "Epoch 46/200\n",
      "60267/60267 [==============================] - 46s 765us/step - loss: 0.0913 - val_loss: 0.0921\n",
      "Epoch 47/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.0914 - val_loss: 0.0921\n",
      "Epoch 48/200\n",
      "60267/60267 [==============================] - 46s 769us/step - loss: 0.0914 - val_loss: 0.0923\n",
      "Epoch 49/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.0911 - val_loss: 0.0921\n",
      "Epoch 50/200\n",
      "60267/60267 [==============================] - 46s 766us/step - loss: 0.0911 - val_loss: 0.0923\n",
      "Epoch 51/200\n",
      "60267/60267 [==============================] - 46s 763us/step - loss: 0.0908 - val_loss: 0.0920\n",
      "Epoch 52/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.0905 - val_loss: 0.0922\n",
      "Epoch 53/200\n",
      "60267/60267 [==============================] - 47s 778us/step - loss: 0.0906 - val_loss: 0.0923\n",
      "Epoch 54/200\n",
      "60267/60267 [==============================] - 46s 768us/step - loss: 0.0910 - val_loss: 0.0924\n",
      "Epoch 55/200\n",
      "60267/60267 [==============================] - 47s 773us/step - loss: 0.0904 - val_loss: 0.0922\n",
      "Epoch 56/200\n",
      "60267/60267 [==============================] - 46s 771us/step - loss: 0.0902 - val_loss: 0.0924\n",
      "Epoch 57/200\n",
      "60267/60267 [==============================] - 47s 772us/step - loss: 0.0897 - val_loss: 0.0922\n",
      "Epoch 58/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.0899 - val_loss: 0.0920\n",
      "Epoch 59/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.0899 - val_loss: 0.0921\n",
      "Epoch 60/200\n",
      "60267/60267 [==============================] - 46s 771us/step - loss: 0.0901 - val_loss: 0.0920\n",
      "Epoch 61/200\n",
      "60267/60267 [==============================] - 46s 771us/step - loss: 0.0900 - val_loss: 0.0921\n",
      "Epoch 62/200\n",
      "60267/60267 [==============================] - 46s 768us/step - loss: 0.0904 - val_loss: 0.0921\n",
      "Epoch 63/200\n",
      "60267/60267 [==============================] - 47s 775us/step - loss: 0.0901 - val_loss: 0.0921\n",
      "Epoch 64/200\n",
      "60267/60267 [==============================] - 46s 769us/step - loss: 0.0899 - val_loss: 0.0921\n",
      "Epoch 65/200\n",
      "60267/60267 [==============================] - 46s 770us/step - loss: 0.0902 - val_loss: 0.0921\n",
      "Epoch 66/200\n",
      "60267/60267 [==============================] - 46s 765us/step - loss: 0.0900 - val_loss: 0.0921\n",
      "Epoch 67/200\n",
      "60267/60267 [==============================] - 46s 769us/step - loss: 0.0897 - val_loss: 0.0921\n",
      "Epoch 68/200\n",
      "60267/60267 [==============================] - 46s 771us/step - loss: 0.0898 - val_loss: 0.0921\n",
      "Epoch 69/200\n",
      "60267/60267 [==============================] - 46s 766us/step - loss: 0.0898 - val_loss: 0.0921\n",
      "Epoch 70/200\n",
      "60267/60267 [==============================] - 47s 772us/step - loss: 0.0893 - val_loss: 0.0921\n",
      "Epoch 71/200\n",
      "60267/60267 [==============================] - 46s 768us/step - loss: 0.0898 - val_loss: 0.0921\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00071: early stopping\n",
      "Train on 60268 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60268/60268 [==============================] - 50s 831us/step - loss: 0.1262 - val_loss: 0.1116\n",
      "Epoch 2/200\n",
      "60268/60268 [==============================] - 46s 771us/step - loss: 0.1085 - val_loss: 0.1044\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.1060 - val_loss: 0.0992\n",
      "Epoch 4/200\n",
      "60268/60268 [==============================] - 47s 779us/step - loss: 0.1040 - val_loss: 0.0991\n",
      "Epoch 5/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.1026 - val_loss: 0.1007\n",
      "Epoch 6/200\n",
      "60268/60268 [==============================] - 47s 779us/step - loss: 0.1016 - val_loss: 0.0966\n",
      "Epoch 7/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.1007 - val_loss: 0.0981\n",
      "Epoch 8/200\n",
      "60268/60268 [==============================] - 47s 784us/step - loss: 0.0999 - val_loss: 0.0964\n",
      "Epoch 9/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0999 - val_loss: 0.0965\n",
      "Epoch 10/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0992 - val_loss: 0.0962\n",
      "Epoch 11/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0988 - val_loss: 0.0968\n",
      "Epoch 12/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0984 - val_loss: 0.0967\n",
      "Epoch 13/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0980 - val_loss: 0.0958\n",
      "Epoch 14/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0982 - val_loss: 0.0957\n",
      "Epoch 15/200\n",
      "60268/60268 [==============================] - 47s 778us/step - loss: 0.0977 - val_loss: 0.0953\n",
      "Epoch 16/200\n",
      "60268/60268 [==============================] - 47s 787us/step - loss: 0.0974 - val_loss: 0.0958\n",
      "Epoch 17/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0966 - val_loss: 0.0966\n",
      "Epoch 18/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0968 - val_loss: 0.0956\n",
      "Epoch 19/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0963 - val_loss: 0.0966\n",
      "Epoch 20/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0963 - val_loss: 0.0960\n",
      "Epoch 21/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0946 - val_loss: 0.0960\n",
      "Epoch 22/200\n",
      "60268/60268 [==============================] - 47s 787us/step - loss: 0.0943 - val_loss: 0.0944\n",
      "Epoch 23/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.0943 - val_loss: 0.0940\n",
      "Epoch 24/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.0946 - val_loss: 0.0944\n",
      "Epoch 25/200\n",
      "60268/60268 [==============================] - 47s 779us/step - loss: 0.0944 - val_loss: 0.0950\n",
      "Epoch 26/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0941 - val_loss: 0.0940\n",
      "Epoch 27/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0942 - val_loss: 0.0952\n",
      "Epoch 28/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0942 - val_loss: 0.0944\n",
      "Epoch 29/200\n",
      "60268/60268 [==============================] - 47s 773us/step - loss: 0.0929 - val_loss: 0.0943\n",
      "Epoch 30/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0929 - val_loss: 0.0943\n",
      "Epoch 31/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0930 - val_loss: 0.0942\n",
      "Epoch 32/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.0928 - val_loss: 0.0940\n",
      "Epoch 33/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0924 - val_loss: 0.0937\n",
      "Epoch 34/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0928 - val_loss: 0.0939\n",
      "Epoch 35/200\n",
      "60268/60268 [==============================] - 47s 779us/step - loss: 0.0925 - val_loss: 0.0940\n",
      "Epoch 36/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0923 - val_loss: 0.0937\n",
      "Epoch 37/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0924 - val_loss: 0.0937\n",
      "Epoch 38/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0925 - val_loss: 0.0938\n",
      "Epoch 39/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0918 - val_loss: 0.0938\n",
      "Epoch 40/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0918 - val_loss: 0.0937\n",
      "Epoch 41/200\n",
      "60268/60268 [==============================] - 47s 784us/step - loss: 0.0922 - val_loss: 0.0939\n",
      "Epoch 42/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0919 - val_loss: 0.0935\n",
      "Epoch 43/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.0920 - val_loss: 0.0940\n",
      "Epoch 44/200\n",
      "60268/60268 [==============================] - 47s 773us/step - loss: 0.0915 - val_loss: 0.0942\n",
      "Epoch 45/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0918 - val_loss: 0.0935\n",
      "Epoch 46/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0914 - val_loss: 0.0937\n",
      "Epoch 47/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0918 - val_loss: 0.0937\n",
      "Epoch 48/200\n",
      "60268/60268 [==============================] - 47s 778us/step - loss: 0.0916 - val_loss: 0.0934\n",
      "Epoch 49/200\n",
      "60268/60268 [==============================] - 46s 770us/step - loss: 0.0916 - val_loss: 0.0936\n",
      "Epoch 50/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0913 - val_loss: 0.0936\n",
      "Epoch 51/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0911 - val_loss: 0.0939\n",
      "Epoch 52/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0913 - val_loss: 0.0936\n",
      "Epoch 53/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.0913 - val_loss: 0.0936\n",
      "Epoch 54/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0915 - val_loss: 0.0937\n",
      "Epoch 55/200\n",
      "60268/60268 [==============================] - 47s 773us/step - loss: 0.0912 - val_loss: 0.0937\n",
      "Epoch 56/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0908 - val_loss: 0.0936\n",
      "Epoch 57/200\n",
      "60268/60268 [==============================] - 47s 774us/step - loss: 0.0910 - val_loss: 0.0937\n",
      "Epoch 58/200\n",
      "60268/60268 [==============================] - 47s 774us/step - loss: 0.0911 - val_loss: 0.0938\n",
      "Epoch 59/200\n",
      "60268/60268 [==============================] - 46s 771us/step - loss: 0.0911 - val_loss: 0.0937\n",
      "Epoch 60/200\n",
      "60268/60268 [==============================] - 47s 778us/step - loss: 0.0910 - val_loss: 0.0936\n",
      "Epoch 61/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0909 - val_loss: 0.0937\n",
      "Epoch 62/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0910 - val_loss: 0.0937\n",
      "Epoch 63/200\n",
      "60268/60268 [==============================] - 47s 772us/step - loss: 0.0910 - val_loss: 0.0937\n",
      "Epoch 64/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0912 - val_loss: 0.0937\n",
      "Epoch 65/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0907 - val_loss: 0.0937\n",
      "Epoch 66/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.0912 - val_loss: 0.0937\n",
      "Epoch 67/200\n",
      "60268/60268 [==============================] - 47s 772us/step - loss: 0.0908 - val_loss: 0.0937\n",
      "Epoch 68/200\n",
      "60268/60268 [==============================] - 47s 773us/step - loss: 0.0906 - val_loss: 0.0937\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00068: early stopping\n",
      "Train on 60268 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60268/60268 [==============================] - 50s 834us/step - loss: 0.1324 - val_loss: 0.1077\n",
      "Epoch 2/200\n",
      "60268/60268 [==============================] - 47s 778us/step - loss: 0.1112 - val_loss: 0.1012\n",
      "Epoch 3/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.1062 - val_loss: 0.1012\n",
      "Epoch 4/200\n",
      "60268/60268 [==============================] - 47s 784us/step - loss: 0.1043 - val_loss: 0.0971\n",
      "Epoch 5/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.1026 - val_loss: 0.0993\n",
      "Epoch 6/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.1019 - val_loss: 0.0972\n",
      "Epoch 7/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.1012 - val_loss: 0.0968\n",
      "Epoch 8/200\n",
      "60268/60268 [==============================] - 47s 778us/step - loss: 0.1005 - val_loss: 0.0956\n",
      "Epoch 9/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60268/60268 [==============================] - 48s 791us/step - loss: 0.0999 - val_loss: 0.0963\n",
      "Epoch 10/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0996 - val_loss: 0.0956\n",
      "Epoch 11/200\n",
      "60268/60268 [==============================] - 48s 794us/step - loss: 0.0989 - val_loss: 0.0954\n",
      "Epoch 12/200\n",
      "60268/60268 [==============================] - 48s 792us/step - loss: 0.0989 - val_loss: 0.0953\n",
      "Epoch 13/200\n",
      "60268/60268 [==============================] - 47s 788us/step - loss: 0.0985 - val_loss: 0.0949\n",
      "Epoch 14/200\n",
      "60268/60268 [==============================] - 48s 792us/step - loss: 0.0981 - val_loss: 0.0959\n",
      "Epoch 15/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0980 - val_loss: 0.0955\n",
      "Epoch 16/200\n",
      "60268/60268 [==============================] - 48s 791us/step - loss: 0.0977 - val_loss: 0.0952\n",
      "Epoch 17/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0976 - val_loss: 0.0967\n",
      "Epoch 18/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0974 - val_loss: 0.0946\n",
      "Epoch 19/200\n",
      "60268/60268 [==============================] - 48s 793us/step - loss: 0.0970 - val_loss: 0.0943\n",
      "Epoch 20/200\n",
      "60268/60268 [==============================] - 48s 793us/step - loss: 0.0970 - val_loss: 0.0941\n",
      "Epoch 21/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 22/200\n",
      "60268/60268 [==============================] - 48s 793us/step - loss: 0.0963 - val_loss: 0.0947\n",
      "Epoch 23/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0969 - val_loss: 0.0948\n",
      "Epoch 24/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0964 - val_loss: 0.0957\n",
      "Epoch 25/200\n",
      "60268/60268 [==============================] - 47s 787us/step - loss: 0.0964 - val_loss: 0.0971\n",
      "Epoch 26/200\n",
      "60268/60268 [==============================] - 48s 791us/step - loss: 0.0949 - val_loss: 0.0937\n",
      "Epoch 27/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0944 - val_loss: 0.0940\n",
      "Epoch 28/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0947 - val_loss: 0.0944\n",
      "Epoch 29/200\n",
      "60268/60268 [==============================] - 48s 788us/step - loss: 0.0944 - val_loss: 0.0937\n",
      "Epoch 30/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0939 - val_loss: 0.0943\n",
      "Epoch 31/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0944 - val_loss: 0.0939\n",
      "Epoch 32/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0936 - val_loss: 0.0937\n",
      "Epoch 33/200\n",
      "60268/60268 [==============================] - 47s 779us/step - loss: 0.0933 - val_loss: 0.0936\n",
      "Epoch 34/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0934 - val_loss: 0.0936\n",
      "Epoch 35/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0930 - val_loss: 0.0935\n",
      "Epoch 36/200\n",
      "60268/60268 [==============================] - 47s 788us/step - loss: 0.0933 - val_loss: 0.0941\n",
      "Epoch 37/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0930 - val_loss: 0.0938\n",
      "Epoch 38/200\n",
      "60268/60268 [==============================] - 47s 787us/step - loss: 0.0927 - val_loss: 0.0941\n",
      "Epoch 39/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0930 - val_loss: 0.0935\n",
      "Epoch 40/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0928 - val_loss: 0.0935\n",
      "Epoch 41/200\n",
      "60268/60268 [==============================] - 47s 778us/step - loss: 0.0922 - val_loss: 0.0936\n",
      "Epoch 42/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0923 - val_loss: 0.0937\n",
      "Epoch 43/200\n",
      "60268/60268 [==============================] - 48s 793us/step - loss: 0.0925 - val_loss: 0.0937\n",
      "Epoch 44/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0923 - val_loss: 0.0937\n",
      "Epoch 45/200\n",
      "60268/60268 [==============================] - 48s 792us/step - loss: 0.0922 - val_loss: 0.0937\n",
      "Epoch 46/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0916 - val_loss: 0.0937\n",
      "Epoch 47/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0919 - val_loss: 0.0937\n",
      "Epoch 48/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0918 - val_loss: 0.0936\n",
      "Epoch 49/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0916 - val_loss: 0.0936\n",
      "Epoch 50/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0918 - val_loss: 0.0935\n",
      "Epoch 51/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0916 - val_loss: 0.0935\n",
      "Epoch 52/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.0914 - val_loss: 0.0935\n",
      "Epoch 53/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0918 - val_loss: 0.0935\n",
      "Epoch 54/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0916 - val_loss: 0.0935\n",
      "Epoch 55/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0919 - val_loss: 0.0935\n",
      "Epoch 56/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0916 - val_loss: 0.0936\n",
      "Epoch 57/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.0916 - val_loss: 0.0935\n",
      "Epoch 58/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0915 - val_loss: 0.0935\n",
      "Epoch 59/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0916 - val_loss: 0.0935\n",
      "Epoch 60/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0915 - val_loss: 0.0936\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00060: early stopping\n",
      "Train on 60268 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60268/60268 [==============================] - 50s 835us/step - loss: 0.1305 - val_loss: 0.1038\n",
      "Epoch 2/200\n",
      "60268/60268 [==============================] - 47s 777us/step - loss: 0.1103 - val_loss: 0.0999\n",
      "Epoch 3/200\n",
      "60268/60268 [==============================] - 47s 779us/step - loss: 0.1074 - val_loss: 0.0975\n",
      "Epoch 4/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.1043 - val_loss: 0.0972\n",
      "Epoch 5/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.1033 - val_loss: 0.0967\n",
      "Epoch 6/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.1028 - val_loss: 0.0954\n",
      "Epoch 7/200\n",
      "60268/60268 [==============================] - 47s 773us/step - loss: 0.1016 - val_loss: 0.0968\n",
      "Epoch 8/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.1010 - val_loss: 0.0952\n",
      "Epoch 9/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.1005 - val_loss: 0.0970\n",
      "Epoch 10/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.0999 - val_loss: 0.0957\n",
      "Epoch 11/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0996 - val_loss: 0.0943\n",
      "Epoch 12/200\n",
      "60268/60268 [==============================] - 46s 760us/step - loss: 0.0991 - val_loss: 0.0948\n",
      "Epoch 13/200\n",
      "60268/60268 [==============================] - 45s 742us/step - loss: 0.0985 - val_loss: 0.0951\n",
      "Epoch 14/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0984 - val_loss: 0.0939\n",
      "Epoch 15/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.0983 - val_loss: 0.0936\n",
      "Epoch 16/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0980 - val_loss: 0.0946\n",
      "Epoch 17/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0978 - val_loss: 0.0936\n",
      "Epoch 18/200\n",
      "60268/60268 [==============================] - 47s 774us/step - loss: 0.0978 - val_loss: 0.0941\n",
      "Epoch 19/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.0972 - val_loss: 0.0939\n",
      "Epoch 20/200\n",
      "60268/60268 [==============================] - 47s 776us/step - loss: 0.0967 - val_loss: 0.0943\n",
      "Epoch 21/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0959 - val_loss: 0.0930\n",
      "Epoch 22/200\n",
      "60268/60268 [==============================] - 47s 773us/step - loss: 0.0958 - val_loss: 0.0932\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0959 - val_loss: 0.0932\n",
      "Epoch 24/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0954 - val_loss: 0.0940\n",
      "Epoch 25/200\n",
      "60268/60268 [==============================] - 47s 784us/step - loss: 0.0955 - val_loss: 0.0938\n",
      "Epoch 26/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0953 - val_loss: 0.0934\n",
      "Epoch 27/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0946 - val_loss: 0.0930\n",
      "Epoch 28/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0942 - val_loss: 0.0933\n",
      "Epoch 29/200\n",
      "60268/60268 [==============================] - 47s 788us/step - loss: 0.0943 - val_loss: 0.0933\n",
      "Epoch 30/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0942 - val_loss: 0.0935\n",
      "Epoch 31/200\n",
      "60268/60268 [==============================] - 48s 793us/step - loss: 0.0941 - val_loss: 0.0934\n",
      "Epoch 32/200\n",
      "60268/60268 [==============================] - 48s 791us/step - loss: 0.0938 - val_loss: 0.0930\n",
      "Epoch 33/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0937 - val_loss: 0.0930\n",
      "Epoch 34/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0937 - val_loss: 0.0931\n",
      "Epoch 35/200\n",
      "60268/60268 [==============================] - 47s 788us/step - loss: 0.0939 - val_loss: 0.0931\n",
      "Epoch 36/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0937 - val_loss: 0.0934\n",
      "Epoch 37/200\n",
      "60268/60268 [==============================] - 47s 784us/step - loss: 0.0935 - val_loss: 0.0931\n",
      "Epoch 38/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0933 - val_loss: 0.0930\n",
      "Epoch 39/200\n",
      "60268/60268 [==============================] - 47s 788us/step - loss: 0.0935 - val_loss: 0.0932\n",
      "Epoch 40/200\n",
      "60268/60268 [==============================] - 47s 787us/step - loss: 0.0933 - val_loss: 0.0930\n",
      "Epoch 41/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0934 - val_loss: 0.0930\n",
      "Epoch 42/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0935 - val_loss: 0.0929\n",
      "Epoch 43/200\n",
      "60268/60268 [==============================] - 47s 784us/step - loss: 0.0928 - val_loss: 0.0931\n",
      "Epoch 44/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0931 - val_loss: 0.0930\n",
      "Epoch 45/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0930 - val_loss: 0.0930\n",
      "Epoch 46/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0934 - val_loss: 0.0930\n",
      "Epoch 47/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0933 - val_loss: 0.0929\n",
      "Epoch 48/200\n",
      "60268/60268 [==============================] - 47s 775us/step - loss: 0.0931 - val_loss: 0.0930\n",
      "Epoch 49/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0931 - val_loss: 0.0930\n",
      "Epoch 50/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0930 - val_loss: 0.0930\n",
      "Epoch 51/200\n",
      "60268/60268 [==============================] - 47s 784us/step - loss: 0.0933 - val_loss: 0.0930\n",
      "Epoch 52/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0935 - val_loss: 0.0930\n",
      "Epoch 53/200\n",
      "60268/60268 [==============================] - 47s 787us/step - loss: 0.0931 - val_loss: 0.0931\n",
      "Epoch 54/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0927 - val_loss: 0.0930\n",
      "Epoch 55/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0930 - val_loss: 0.0930\n",
      "Epoch 56/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0934 - val_loss: 0.0930\n",
      "Epoch 57/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0929 - val_loss: 0.0930\n",
      "Epoch 58/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0930 - val_loss: 0.0930\n",
      "Epoch 59/200\n",
      "60268/60268 [==============================] - 47s 783us/step - loss: 0.0932 - val_loss: 0.0930\n",
      "Epoch 60/200\n",
      "60268/60268 [==============================] - 47s 774us/step - loss: 0.0931 - val_loss: 0.0930\n",
      "Epoch 61/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0930 - val_loss: 0.0930\n",
      "Epoch 62/200\n",
      "60268/60268 [==============================] - 47s 778us/step - loss: 0.0930 - val_loss: 0.0930\n",
      "Epoch 63/200\n",
      "60268/60268 [==============================] - 47s 782us/step - loss: 0.0933 - val_loss: 0.0930\n",
      "Epoch 64/200\n",
      "60268/60268 [==============================] - 47s 779us/step - loss: 0.0930 - val_loss: 0.0930\n",
      "Epoch 65/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0930 - val_loss: 0.0930\n",
      "Epoch 66/200\n",
      "60268/60268 [==============================] - 47s 781us/step - loss: 0.0931 - val_loss: 0.0930\n",
      "Epoch 67/200\n",
      "60268/60268 [==============================] - 47s 780us/step - loss: 0.0932 - val_loss: 0.0930\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00067: early stopping\n",
      "Train on 60268 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "60268/60268 [==============================] - 51s 849us/step - loss: 0.1283 - val_loss: 0.1060\n",
      "Epoch 2/200\n",
      "60268/60268 [==============================] - 48s 794us/step - loss: 0.1108 - val_loss: 0.1007\n",
      "Epoch 3/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.1068 - val_loss: 0.1032\n",
      "Epoch 4/200\n",
      "60268/60268 [==============================] - 48s 800us/step - loss: 0.1044 - val_loss: 0.0997\n",
      "Epoch 5/200\n",
      "60268/60268 [==============================] - 47s 784us/step - loss: 0.1028 - val_loss: 0.0981\n",
      "Epoch 6/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.1023 - val_loss: 0.1003\n",
      "Epoch 7/200\n",
      "60268/60268 [==============================] - 48s 792us/step - loss: 0.1015 - val_loss: 0.0973\n",
      "Epoch 8/200\n",
      "60268/60268 [==============================] - 48s 797us/step - loss: 0.1005 - val_loss: 0.0959\n",
      "Epoch 9/200\n",
      "60268/60268 [==============================] - 47s 788us/step - loss: 0.1005 - val_loss: 0.0957\n",
      "Epoch 10/200\n",
      "60268/60268 [==============================] - 48s 803us/step - loss: 0.0994 - val_loss: 0.0954\n",
      "Epoch 11/200\n",
      "60268/60268 [==============================] - 48s 800us/step - loss: 0.0992 - val_loss: 0.0972\n",
      "Epoch 12/200\n",
      "60268/60268 [==============================] - 47s 786us/step - loss: 0.0988 - val_loss: 0.0964\n",
      "Epoch 13/200\n",
      "60268/60268 [==============================] - 48s 797us/step - loss: 0.0980 - val_loss: 0.0949\n",
      "Epoch 14/200\n",
      "60268/60268 [==============================] - 48s 791us/step - loss: 0.0979 - val_loss: 0.0958\n",
      "Epoch 15/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0973 - val_loss: 0.0946\n",
      "Epoch 16/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0970 - val_loss: 0.0947\n",
      "Epoch 17/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0969 - val_loss: 0.0954\n",
      "Epoch 18/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0966 - val_loss: 0.0940\n",
      "Epoch 19/200\n",
      "60268/60268 [==============================] - 48s 800us/step - loss: 0.0958 - val_loss: 0.0940\n",
      "Epoch 20/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0961 - val_loss: 0.0956\n",
      "Epoch 21/200\n",
      "60268/60268 [==============================] - 47s 787us/step - loss: 0.0956 - val_loss: 0.0954\n",
      "Epoch 22/200\n",
      "60268/60268 [==============================] - 48s 789us/step - loss: 0.0954 - val_loss: 0.0943\n",
      "Epoch 23/200\n",
      "60268/60268 [==============================] - 48s 797us/step - loss: 0.0948 - val_loss: 0.0948\n",
      "Epoch 24/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0939 - val_loss: 0.0936\n",
      "Epoch 25/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0936 - val_loss: 0.0947\n",
      "Epoch 26/200\n",
      "60268/60268 [==============================] - 48s 792us/step - loss: 0.0929 - val_loss: 0.0935\n",
      "Epoch 27/200\n",
      "60268/60268 [==============================] - 48s 791us/step - loss: 0.0930 - val_loss: 0.0934\n",
      "Epoch 28/200\n",
      "60268/60268 [==============================] - 48s 792us/step - loss: 0.0928 - val_loss: 0.0933\n",
      "Epoch 29/200\n",
      "60268/60268 [==============================] - 47s 785us/step - loss: 0.0927 - val_loss: 0.0942\n",
      "Epoch 30/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60268/60268 [==============================] - 49s 809us/step - loss: 0.0925 - val_loss: 0.0942\n",
      "Epoch 31/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0928 - val_loss: 0.0932\n",
      "Epoch 32/200\n",
      "60268/60268 [==============================] - 48s 801us/step - loss: 0.0923 - val_loss: 0.0938\n",
      "Epoch 33/200\n",
      "60268/60268 [==============================] - 48s 803us/step - loss: 0.0923 - val_loss: 0.0939\n",
      "Epoch 34/200\n",
      "60268/60268 [==============================] - 49s 806us/step - loss: 0.0919 - val_loss: 0.0933\n",
      "Epoch 35/200\n",
      "60268/60268 [==============================] - 49s 805us/step - loss: 0.0921 - val_loss: 0.0939\n",
      "Epoch 36/200\n",
      "60268/60268 [==============================] - 49s 807us/step - loss: 0.0921 - val_loss: 0.0936\n",
      "Epoch 37/200\n",
      "60268/60268 [==============================] - 48s 801us/step - loss: 0.0912 - val_loss: 0.0938\n",
      "Epoch 38/200\n",
      "60268/60268 [==============================] - 49s 806us/step - loss: 0.0910 - val_loss: 0.0937\n",
      "Epoch 39/200\n",
      "60268/60268 [==============================] - 49s 805us/step - loss: 0.0911 - val_loss: 0.0938\n",
      "Epoch 40/200\n",
      "60268/60268 [==============================] - 49s 811us/step - loss: 0.0907 - val_loss: 0.0934\n",
      "Epoch 41/200\n",
      "60268/60268 [==============================] - 49s 805us/step - loss: 0.0908 - val_loss: 0.0932\n",
      "Epoch 42/200\n",
      "60268/60268 [==============================] - 49s 805us/step - loss: 0.0900 - val_loss: 0.0933\n",
      "Epoch 43/200\n",
      "60268/60268 [==============================] - 48s 803us/step - loss: 0.0901 - val_loss: 0.0934\n",
      "Epoch 44/200\n",
      "60268/60268 [==============================] - 49s 805us/step - loss: 0.0901 - val_loss: 0.0930\n",
      "Epoch 45/200\n",
      "60268/60268 [==============================] - 49s 807us/step - loss: 0.0897 - val_loss: 0.0932\n",
      "Epoch 46/200\n",
      "60268/60268 [==============================] - 49s 807us/step - loss: 0.0899 - val_loss: 0.0929\n",
      "Epoch 47/200\n",
      "60268/60268 [==============================] - 48s 803us/step - loss: 0.0898 - val_loss: 0.0931\n",
      "Epoch 48/200\n",
      "60268/60268 [==============================] - 48s 798us/step - loss: 0.0897 - val_loss: 0.0928\n",
      "Epoch 49/200\n",
      "60268/60268 [==============================] - 48s 798us/step - loss: 0.0895 - val_loss: 0.0929\n",
      "Epoch 50/200\n",
      "60268/60268 [==============================] - 49s 806us/step - loss: 0.0896 - val_loss: 0.0927\n",
      "Epoch 51/200\n",
      "60268/60268 [==============================] - 48s 804us/step - loss: 0.0893 - val_loss: 0.0931\n",
      "Epoch 52/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0894 - val_loss: 0.0929\n",
      "Epoch 53/200\n",
      "60268/60268 [==============================] - 48s 798us/step - loss: 0.0895 - val_loss: 0.0928\n",
      "Epoch 54/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0889 - val_loss: 0.0927\n",
      "Epoch 55/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0893 - val_loss: 0.0928\n",
      "Epoch 56/200\n",
      "60268/60268 [==============================] - 48s 793us/step - loss: 0.0888 - val_loss: 0.0928\n",
      "Epoch 57/200\n",
      "60268/60268 [==============================] - 48s 802us/step - loss: 0.0886 - val_loss: 0.0928\n",
      "Epoch 58/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0887 - val_loss: 0.0927\n",
      "Epoch 59/200\n",
      "60268/60268 [==============================] - 48s 801us/step - loss: 0.0889 - val_loss: 0.0927\n",
      "Epoch 60/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0885 - val_loss: 0.0926\n",
      "Epoch 61/200\n",
      "60268/60268 [==============================] - 49s 805us/step - loss: 0.0887 - val_loss: 0.0927\n",
      "Epoch 62/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0888 - val_loss: 0.0927\n",
      "Epoch 63/200\n",
      "60268/60268 [==============================] - 48s 800us/step - loss: 0.0889 - val_loss: 0.0927\n",
      "Epoch 64/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0887 - val_loss: 0.0927\n",
      "Epoch 65/200\n",
      "60268/60268 [==============================] - 48s 805us/step - loss: 0.0886 - val_loss: 0.0927\n",
      "Epoch 66/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0888 - val_loss: 0.0927\n",
      "Epoch 67/200\n",
      "60268/60268 [==============================] - 48s 801us/step - loss: 0.0888 - val_loss: 0.0927\n",
      "Epoch 68/200\n",
      "60268/60268 [==============================] - 48s 794us/step - loss: 0.0884 - val_loss: 0.0927\n",
      "Epoch 69/200\n",
      "60268/60268 [==============================] - 48s 803us/step - loss: 0.0887 - val_loss: 0.0927\n",
      "Epoch 70/200\n",
      "60268/60268 [==============================] - 48s 794us/step - loss: 0.0883 - val_loss: 0.0927\n",
      "Epoch 71/200\n",
      "60268/60268 [==============================] - 48s 794us/step - loss: 0.0889 - val_loss: 0.0927\n",
      "Epoch 72/200\n",
      "60268/60268 [==============================] - 48s 796us/step - loss: 0.0884 - val_loss: 0.0927\n",
      "Epoch 73/200\n",
      "60268/60268 [==============================] - 48s 797us/step - loss: 0.0883 - val_loss: 0.0927\n",
      "Epoch 74/200\n",
      "60268/60268 [==============================] - 48s 793us/step - loss: 0.0884 - val_loss: 0.0927\n",
      "Epoch 75/200\n",
      "60268/60268 [==============================] - 48s 792us/step - loss: 0.0885 - val_loss: 0.0927\n",
      "Epoch 76/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0885 - val_loss: 0.0927\n",
      "Epoch 77/200\n",
      "60268/60268 [==============================] - 48s 798us/step - loss: 0.0882 - val_loss: 0.0927\n",
      "Epoch 78/200\n",
      "60268/60268 [==============================] - 48s 798us/step - loss: 0.0881 - val_loss: 0.0927\n",
      "Epoch 79/200\n",
      "60268/60268 [==============================] - 48s 795us/step - loss: 0.0883 - val_loss: 0.0927\n",
      "Epoch 80/200\n",
      "60268/60268 [==============================] - 48s 790us/step - loss: 0.0887 - val_loss: 0.0927\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00080: early stopping\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "PATIENCE = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "nn_score = 0.0\n",
    "\n",
    "for train_index, test_index in skf.split(X, (y * 100).astype(int)):\n",
    "    \n",
    "    X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "    y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "    \n",
    "    docs_train, docs_test = data[train_index], data[test_index]\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=RANDOM_SEED)\n",
    "    tridx, vidx = next(sss.split(X_train, (y_train * 100).astype(int)))\n",
    "    X_train, X_valid = X_train[tridx], X_train[vidx]\n",
    "    y_train, y_valid = y_train[tridx], y_train[vidx]\n",
    "    docs_train, docs_valid = docs_train[tridx], docs_train[vidx]\n",
    "    \n",
    "    # Build model\n",
    "    model, band_embedding_model, influencer_embedding_model = build_NN_model(dropout=0.3)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        mode='min', \n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                                  patience=5)\n",
    "    \n",
    "    # Fit\n",
    "    model.fit([X_train[:, i_data_idx], X_train[:, b_data_idx], X_train[:, i_kind_idx], docs_train], \n",
    "              y_train,\n",
    "              validation_data=([X_valid[:, i_data_idx], \n",
    "                                X_valid[:, b_data_idx], \n",
    "                                X_valid[:, i_kind_idx],\n",
    "                                docs_valid], \n",
    "                               y_valid), \n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS,\n",
    "              callbacks=[es, reduce_lr],\n",
    "              verbose=1)\n",
    "    \n",
    "    nn_score += np.sqrt(mean_squared_error(\n",
    "        model.predict([X_test[:, i_data_idx], X_test[:, b_data_idx], X_test[:, i_kind_idx], docs_test]), \n",
    "        y_test\n",
    "    ))\n",
    "\n",
    "nn_score /= N_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3080378590783084\n"
     ]
    }
   ],
   "source": [
    "print(nn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60267 samples, validate on 6697 samples\n",
      "Epoch 1/200\n",
      "17824/60267 [=======>......................] - ETA: 21s - loss: 0.1418"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ac7e300b8aac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m               verbose=1)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     nn_score += np.sqrt(mean_squared_error(\n",
      "\u001b[0;32m~/.virtualenvs/groover/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/groover/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/groover/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/groover/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/groover/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "PATIENCE = 15\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "nn_score = 0.0\n",
    "\n",
    "for train_index, test_index in skf.split(X, (y * 100).astype(int)):\n",
    "    \n",
    "    X_train, X_test = X.values[train_index], X.values[test_index]\n",
    "    y_train, y_test = y.values[train_index], y.values[test_index]\n",
    "    \n",
    "    docs_train, docs_test = data[train_index], data[test_index]\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=RANDOM_SEED)\n",
    "    tridx, vidx = next(sss.split(X_train, (y_train * 100).astype(int)))\n",
    "    X_train, X_valid = X_train[tridx], X_train[vidx]\n",
    "    y_train, y_valid = y_train[tridx], y_train[vidx]\n",
    "    docs_train, docs_valid = docs_train[tridx], docs_train[vidx]\n",
    "    \n",
    "    # Build model\n",
    "    model, band_embedding_model, influencer_embedding_model = build_NN_model(dropout=0.1)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    es = EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        mode='min', \n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit\n",
    "    model.fit([X_train[:, i_data_idx], X_train[:, b_data_idx], X_train[:, i_kind_idx], docs_train], \n",
    "              y_train,\n",
    "              validation_data=([X_valid[:, i_data_idx], \n",
    "                                X_valid[:, b_data_idx], \n",
    "                                X_valid[:, i_kind_idx],\n",
    "                                docs_valid], \n",
    "                               y_valid), \n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS,\n",
    "              callbacks=[es],\n",
    "              verbose=1)\n",
    "    \n",
    "    nn_score += np.sqrt(mean_squared_error(\n",
    "        model.predict([X_test[:, i_data_idx], X_test[:, b_data_idx], X_test[:, i_kind_idx], docs_test]), \n",
    "        y_test\n",
    "    ))\n",
    "\n",
    "nn_score /= N_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3077258867925123\n"
     ]
    }
   ],
   "source": [
    "print(nn_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groover",
   "language": "python",
   "name": "groover"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
